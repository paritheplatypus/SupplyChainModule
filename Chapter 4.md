# 3D Printing Job Scheduling

Job scheduling concerns the assignment and sequencing of tasks on finite resources to optimize a performance criterion, most commonly the **makespan** (the time at which all jobs finish). The problem pervades manufacturing (machine centers, assembly, additive manufacturing), cloud and HPC (batch queues, GPU farms), and services (operating rooms, call centers). Even in its simplest forms—such as assigning independent jobs with known processing times to identical parallel machines—the number of feasible schedules grows factorially in the number of jobs, placing the problem among the classic NP‑hard families. Exact methods can certify optimality for small instances, but they become computationally prohibitive as systems scale or when decisions must be made online.

This chapter develops a pragmatic approach based on **Simulated Annealing** (SA) to minimize makespan on identical parallel machines. SA operates over permutations of job indices; an evaluation function reconstructs machine loads by greedily dispatching jobs to the currently least‑loaded machine, and the makespan is the resulting completion time. Candidates are generated by local moves (e.g., random pair swaps), and the annealing schedule probabilistically accepts uphill moves in early phases to escape local minima. This metaheuristic is attractive precisely because it respects the problem’s combinatorial structure while remaining simple, tunable, and robust. Our implementation emphasizes clarity and auditable integration with Airflow so that experiments can be repeated, logged, and compared across the cohort.

We separate **algorithm code** from **orchestration**. The SA solver is a plain Python script that can be run independently or invoked by an Airflow DAG. The DAG’s first task computes a schedule and pushes its JSON result to XCom; the second task logs or post‑processes the result. This separation mirrors industry practice: algorithms evolve, but pipelines provide stability, observability, and recoverability.

## Put the SA script and DAG in place
```bash
SCRIPTS_DIR="/opt/airflow/scripts"
mkdir -p "$SCRIPTS_DIR"
tee "$SCRIPTS_DIR/job_scheduling_sa.py" >/dev/null <<'PY'
#!/usr/bin/env python3
# job_scheduling_sa.py
from __future__ import annotations
import random, math, json
from typing import List

def eval_makespan(order: List[int], proc: List[int], m: int) -> int:
    loads = [0]*m
    for job_idx in order:
        k = min(range(m), key=lambda i: loads[i])
        loads[k] += proc[job_idx]
    return max(loads)

def neighbor(order: List[int]) -> List[int]:
    if len(order) < 2: return order[:]
    i, j = sorted(random.sample(range(len(order)), 2))
    new = order[:]
    new[i], new[j] = new[j], new[i]
    return new

def sa_schedule(proc: List[int], m: int, seed: int = 42,
                t0: float = 1.0, alpha: float = 0.995, steps: int = 5000):
    random.seed(seed)
    curr = list(range(len(proc)))
    random.shuffle(curr)
    curr_cost = eval_makespan(curr, proc, m)
    best, best_cost = curr[:], curr_cost
    T = t0
    for _ in range(steps):
        cand = neighbor(curr)
        cand_cost = eval_makespan(cand, proc, m)
        d = cand_cost - curr_cost
        if d < 0 or random.random() < math.exp(-d/max(T,1e-9)):
            curr, curr_cost = cand, cand_cost
            if curr_cost < best_cost:
                best, best_cost = curr[:], curr_cost
        T *= alpha
    return {"order": best, "makespan": best_cost}

if __name__ == "__main__":
    jobs = [7, 3, 8, 2, 5, 9, 4]
    machines = 3
    print(json.dumps(sa_schedule(jobs, machines)))
PY
chmod +x "$SCRIPTS_DIR/job_scheduling_sa.py"

DAGS_DIR="$(airflow config get-value core dags_folder 2>/dev/null || echo ${AIRFLOW_HOME:-$HOME/airflow}/dags)"
tee "$DAGS_DIR/job_scheduling_dag.py" >/dev/null <<'PY'
from datetime import timedelta
import json, pendulum, os, subprocess
from airflow import DAG
from airflow.operators.python import PythonOperator

RESULTS_DIR = "/opt/airflow/results"
os.makedirs(RESULTS_DIR, exist_ok=True)

def run_sa(**context):
    script_path = os.environ.get("SA_SCRIPT_PATH", "/opt/airflow/scripts/job_scheduling_sa.py")
    output = subprocess.check_output(["python3", script_path], text=True)
    result = json.loads(output)
    outpath = os.path.join(RESULTS_DIR, "job_scheduling_result.json")
    with open(outpath, "w") as f:
        json.dump(result, f, indent=4)
    print(f"Saved SA result to: {outpath}")
    context["ti"].xcom_push(key="sa_result", value=result)

def log_result(**context):
    path = os.path.join(RESULTS_DIR, "job_scheduling_result.json")
    if os.path.exists(path):
        with open(path) as f:
            print("SA RESULT FILE CONTENTS:\n" + f.read())
    else:
        print("No result file found at", path)

default_args = {
    "owner": "airflow",
    "retries": 0,
    "retry_delay": timedelta(minutes=1),
}

with DAG(
    dag_id="job_scheduling_sa",
    start_date=pendulum.datetime(2024, 10, 1, tz="UTC"),
    schedule=None,
    catchup=False,
    default_args=default_args,
    tags=["chapter4", "sa", "scheduling"],
) as dag:
    run = PythonOperator(task_id="run_sa", python_callable=run_sa)
    log = PythonOperator(task_id="log_result", python_callable=log_result)
    run >> log
PY

airflow dags list | grep job_scheduling_sa || true
airflow tasks test job_scheduling_sa run_sa 2024-10-01
airflow tasks test job_scheduling_sa log_result 2024-10-01
cat /opt/airflow/results/job_scheduling_result.json
```
